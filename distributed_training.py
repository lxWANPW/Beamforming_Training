#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ä½¿ç”¨DistributedDataParallelè¿›è¡Œæ›´é«˜æ•ˆçš„å¤šGPUè®­ç»ƒ
ä½¿ç”¨æ–¹æ³•ï¼š
python -m torch.distributed.launch --nproc_per_node=2 distributed_training.py
æˆ–è€…
torchrun --nproc_per_node=2 distributed_training.py
"""

import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from beamforming_model_fixed_v2 import *
import argparse

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    # è·å–ç¯å¢ƒå˜é‡
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )
    
    # è®¾ç½®CUDAè®¾å¤‡
    torch.cuda.set_device(local_rank)
    
    return rank, local_rank, world_size

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()

def distributed_train_model(model, train_loader, val_loader, num_epochs=200, lr=1e-4, local_rank=0, rank=0):
    """åˆ†å¸ƒå¼è®­ç»ƒå‡½æ•°"""
    device = torch.device(f'cuda:{local_rank}')
    model = model.to(device)
    
    # åŒ…è£…ä¸ºDDPæ¨¡å‹
    model = DDP(model, device_ids=[local_rank], output_device=local_rank)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=1e-4,
        eps=1e-8,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=20,
        T_mult=2,
        eta_min=lr/100
    )
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()
    
    # è®­ç»ƒå‚æ•°
    accumulation_steps = 2
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 50
    
    if rank == 0:
        print(f"ğŸš€ å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ (Rank {rank}):")
        print(f"   â€¢ åˆå§‹å­¦ä¹ ç‡: {lr}")
        print(f"   â€¢ è®¾å¤‡: {device}")
        print(f"   â€¢ ä¸–ç•Œå¤§å°: {dist.get_world_size()}")
    
    for epoch in range(num_epochs):
        # è®¾ç½®epochç”¨äºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        train_loader.sampler.set_epoch(epoch)
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss_epoch = 0
        train_theta_loss = 0
        train_phi_loss = 0
        num_batches = 0
        
        if rank == 0:
            train_progress = tqdm(train_loader, desc=f"ğŸ¯ è®­ç»ƒ Epoch {epoch+1}/{num_epochs}", leave=False)
        else:
            train_progress = train_loader
        
        optimizer.zero_grad()
        
        for batch_idx, (H_input, theta_label, phi_label, H_complex) in enumerate(train_progress):
            try:
                H_input = H_input.to(device, non_blocking=True)
                theta_label = theta_label.to(device, non_blocking=True)
                phi_label = phi_label.to(device, non_blocking=True)
                
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast():
                    predictions = model(H_input)
                    loss, theta_loss, phi_loss = ultra_loss_function(
                        predictions, (theta_label, phi_label), loss_type='ultra_focal'
                    )
                    loss = loss / accumulation_steps
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦ç´¯ç§¯å’Œä¼˜åŒ–
                if (batch_idx + 1) % accumulation_steps == 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                
                train_loss_epoch += loss.item() * accumulation_steps
                train_theta_loss += theta_loss.item()
                train_phi_loss += phi_loss.item()
                num_batches += 1
                
                if rank == 0:
                    current_lr = optimizer.param_groups[0]['lr']
                    train_progress.set_postfix({
                        'Loss': f'{loss.item() * accumulation_steps:.4f}',
                        'Î¸': f'{theta_loss.item():.4f}',
                        'Ï†': f'{phi_loss.item():.4f}',
                        'LR': f'{current_lr:.2e}'
                    })
                
            except Exception as e:
                if rank == 0:
                    print(f"âŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
        
        scheduler.step()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss_epoch = 0
        val_theta_loss = 0
        val_phi_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            if rank == 0:
                val_progress = tqdm(val_loader, desc="ğŸ” éªŒè¯", leave=False)
            else:
                val_progress = val_loader
                
            for H_input, theta_label, phi_label, H_complex in val_progress:
                try:
                    H_input = H_input.to(device, non_blocking=True)
                    theta_label = theta_label.to(device, non_blocking=True)
                    phi_label = phi_label.to(device, non_blocking=True)
                    
                    with torch.cuda.amp.autocast():
                        predictions = model(H_input)
                        loss, theta_loss, phi_loss = ultra_loss_function(
                            predictions, (theta_label, phi_label), loss_type='ultra_focal'
                        )
                    
                    val_loss_epoch += loss.item()
                    val_theta_loss += theta_loss.item()
                    val_phi_loss += phi_loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    continue
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹çš„éªŒè¯æŸå¤±
        if val_batches > 0:
            val_loss_tensor = torch.tensor([val_loss_epoch / val_batches], device=device)
            dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.AVG)
            val_loss_avg = val_loss_tensor.item()
        else:
            val_loss_avg = float('inf')
        
        # è®­ç»ƒæŸå¤±
        if num_batches > 0:
            train_loss_tensor = torch.tensor([train_loss_epoch / num_batches], device=device)
            dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.AVG)
            train_loss_avg = train_loss_tensor.item()
        else:
            train_loss_avg = float('inf')
        
        train_losses.append(train_loss_avg)
        val_losses.append(val_loss_avg)
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°å’Œä¿å­˜
        if rank == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"ğŸ”¥ Epoch {epoch+1:3d}: "
                  f"Train={train_loss_avg:.6f} | "
                  f"Val={val_loss_avg:.6f} | "
                  f"LR={current_lr:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss_avg < best_val_loss:
                best_val_loss = val_loss_avg
                patience_counter = 0
                torch.save(model.module.state_dict(), 'best_distributed_model.pth')
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss_avg:.6f})")
            else:
                patience_counter += 1
            
            if patience_counter >= early_stop_patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
        
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        dist.barrier()
    
    return train_losses, val_losses

def main_distributed():
    """åˆ†å¸ƒå¼è®­ç»ƒä¸»å‡½æ•°"""
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, local_rank, world_size = setup_distributed()
    
    try:
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°åˆå§‹ä¿¡æ¯
        if rank == 0:
            print(f"ğŸ–¥ï¸  åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–:")
            print(f"   â€¢ æ€»è¿›ç¨‹æ•°: {world_size}")
            print(f"   â€¢ å½“å‰rank: {rank}")
            print(f"   â€¢ æœ¬åœ°rank: {local_rank}")
        
        # æ•°æ®è·¯å¾„
        channel_folder = "samples_data/channel_data_opt_20250702_143327"
        angle_folder = "samples_data/angle_data_opt_20250702_143327"
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = BeamformingDataset(channel_folder, angle_folder)
        
        # æ•°æ®é›†åˆ’åˆ†
        total_samples = len(dataset)
        train_size = 800
        indices = list(range(total_samples))
        np.random.seed(42)
        np.random.shuffle(indices)
        
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]
        
        val_size = int(train_size * 0.25)
        train_final_indices = train_indices[val_size:]
        val_indices = train_indices[:val_size]
        
        # åˆ›å»ºæ•°æ®å­é›†
        train_dataset = torch.utils.data.Subset(dataset, train_final_indices)
        val_dataset = torch.utils.data.Subset(dataset, val_indices)
        
        # åˆ†å¸ƒå¼é‡‡æ ·å™¨
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )
        
        val_sampler = DistributedSampler(
            val_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=False
        )
        
        # æ•°æ®åŠ è½½å™¨
        batch_size = 4  # æ¯ä¸ªè¿›ç¨‹çš„æ‰¹æ¬¡å¤§å°
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            sampler=train_sampler,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            sampler=val_sampler,
            num_workers=4,
            pin_memory=True
        )
        
        # åˆ›å»ºæ¨¡å‹
        if rank == 0:
            print("ğŸ—ï¸  åˆ›å»ºå¢å¼ºæ¨¡å‹...")
        model = create_enhanced_model()
        
        # åˆ†å¸ƒå¼è®­ç»ƒ
        if rank == 0:
            print("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ...")
        
        train_losses, val_losses = distributed_train_model(
            model, train_loader, val_loader,
            num_epochs=200,
            lr=1e-4,
            local_rank=local_rank,
            rank=rank
        )
        
        if rank == 0:
            print("ğŸ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼æ³¢æŸæˆå½¢è®­ç»ƒ')
    parser.add_argument('--local_rank', type=int, default=0, help='æœ¬åœ°GPU rank')
    args = parser.parse_args()
    
    main_distributed()
