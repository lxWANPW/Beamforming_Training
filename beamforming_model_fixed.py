import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import scipy.io as sio
import h5py
import os
import glob
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import math
from tqdm import tqdm
import warnings

class BeamformingDataset(Dataset):
    """ä¿®å¤ç‰ˆå¤©çº¿é˜µåˆ—ä¿¡é“æ•°æ®é›†"""
    def __init__(self, channel_folder, angle_folder, transform=None):
        self.channel_folder = channel_folder
        self.angle_folder = angle_folder
        self.transform = transform
        
        # è·å–ä¿¡é“æ•°æ®æ–‡ä»¶åˆ—è¡¨å¹¶æ’åº
        self.channel_files = sorted(glob.glob(os.path.join(channel_folder, '*.mat')))
        print(f"æ‰¾åˆ°ä¿¡é“æ•°æ®æ–‡ä»¶: {len(self.channel_files)} ä¸ª")
        
        # åŠ è½½è§’åº¦æ•°æ®ï¼ˆæ‰€æœ‰æ ·æœ¬çš„è§’åº¦ä¿¡æ¯åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼‰
        angle_files = glob.glob(os.path.join(angle_folder, '*.mat'))
        if len(angle_files) > 0:
            try:
                # å°è¯•ä½¿ç”¨ h5py è¯»å–
                with h5py.File(angle_files[0], 'r') as f:
                    all_theta = np.array(f['all_theta'])  # (50, 1000) or (1000, 50)
                    all_phi = np.array(f['all_phi'])      # (50, 1000) or (1000, 50)
                    
                    # æ£€æŸ¥ç»´åº¦å¹¶è°ƒæ•´
                    if all_theta.shape[0] == 50 and all_theta.shape[1] == 1000:
                        self.theta_array = all_theta.T  # (1000, 50)
                        self.phi_array = all_phi.T      # (1000, 50)
                    else:
                        self.theta_array = all_theta    # å‡è®¾å·²ç»æ˜¯(1000, 50)
                        self.phi_array = all_phi
                    
                    self.num_samples = self.theta_array.shape[0]
                    self.num_time_samples = self.theta_array.shape[1]
                    
                    print(f"è§’åº¦æ•°æ®å½¢çŠ¶: theta={self.theta_array.shape}, phi={self.phi_array.shape}")
                    
            except Exception as e:
                print(f"h5py è¯»å–å¤±è´¥: {e}")
                # ä½¿ç”¨ scipy.io è¯»å–
                try:
                    angle_data = sio.loadmat(angle_files[0])
                    # æ£€æŸ¥å¯èƒ½çš„é”®å
                    possible_keys = ['all_theta', 'theta_array', 'theta']
                    theta_key = None
                    phi_key = None
                    
                    for key in angle_data.keys():
                        if 'theta' in key.lower():
                            theta_key = key
                        if 'phi' in key.lower():
                            phi_key = key
                    
                    if theta_key and phi_key:
                        all_theta = angle_data[theta_key]
                        all_phi = angle_data[phi_key]
                        
                        if all_theta.shape[0] == 50 and all_theta.shape[1] == 1000:
                            self.theta_array = all_theta.T
                            self.phi_array = all_phi.T
                        else:
                            self.theta_array = all_theta
                            self.phi_array = all_phi
                            
                        self.num_samples = self.theta_array.shape[0]
                        self.num_time_samples = self.theta_array.shape[1]
                    else:
                        raise ValueError("æœªæ‰¾åˆ°thetaå’Œphiæ•°æ®")
                        
                except Exception as e2:
                    print(f"scipy.io è¯»å–ä¹Ÿå¤±è´¥: {e2}")
                    print(f"è§’åº¦æ–‡ä»¶å†…å®¹: {list(angle_data.keys()) if 'angle_data' in locals() else 'Unknown'}")
                    raise e2
        
        # ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œä½¿ç”¨è¾ƒå°çš„æ•°é‡
        self.actual_num_samples = min(len(self.channel_files), self.num_samples)
        print(f"å®é™…ä½¿ç”¨çš„æ ·æœ¬æ•°é‡: {self.actual_num_samples}")
        
    def __len__(self):
        return self.actual_num_samples
    
    def __getitem__(self, idx):
        if idx >= self.actual_num_samples:
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ {self.actual_num_samples}")
            
        # åŠ è½½ä¿¡é“æ•°æ®
        try:
            # å°è¯•ä½¿ç”¨ h5py è¯»å–
            with h5py.File(self.channel_files[idx], 'r') as f:
                H_real = np.array(f['H_real'])  # (50, 256, 256)
                H_imag = np.array(f['H_imag'])  # (50, 256, 256)
                
                # è½¬ç½®ä»¥åŒ¹é…é¢„æœŸæ ¼å¼ [256, 256, 50]
                H_real = H_real.transpose(1, 2, 0)  # (256, 256, 50)
                H_imag = H_imag.transpose(1, 2, 0)  # (256, 256, 50)
                
        except Exception as e:
            # ä½¿ç”¨ scipy.io è¯»å–
            try:
                channel_data = sio.loadmat(self.channel_files[idx])
                H_real = channel_data['H_real']  # [256, 256, 50]
                H_imag = channel_data['H_imag']
            except Exception as e2:
                print(f"æ— æ³•è¯»å–ä¿¡é“æ•°æ®æ–‡ä»¶ {self.channel_files[idx]}: {e2}")
                raise e2
        
        # ç»„åˆå®éƒ¨å’Œè™šéƒ¨
        H_complex = H_real + 1j * H_imag
        
        # æ”¹è¿›çš„æ•°æ®å½’ä¸€åŒ–ï¼ˆé¿å…æå€¼ï¼‰
        H_real_std = np.std(H_real)
        H_imag_std = np.std(H_imag)
        
        if H_real_std > 1e-8:
            H_real_norm = (H_real - np.mean(H_real)) / H_real_std
        else:
            H_real_norm = H_real
            
        if H_imag_std > 1e-8:
            H_imag_norm = (H_imag - np.mean(H_imag)) / H_imag_std
        else:
            H_imag_norm = H_imag
        
        # è£å‰ªæå€¼ä»¥é¿å…æ¢¯åº¦çˆ†ç‚¸
        H_real_norm = np.clip(H_real_norm, -10, 10)
        H_imag_norm = np.clip(H_imag_norm, -10, 10)
        
        # è½¬æ¢ä¸ºPyTorch tensor
        H_input = np.stack([H_real_norm, H_imag_norm], axis=0)  # [2, 256, 256, 50]
        H_input = torch.FloatTensor(H_input)
        
        # æ”¹è¿›çš„è§’åº¦æ ‡ç­¾å½’ä¸€åŒ–
        theta_rad = self.theta_array[idx] * np.pi / 180  # è½¬æ¢ä¸ºå¼§åº¦
        phi_rad = self.phi_array[idx] * np.pi / 180
        
        # ä½¿ç”¨æ›´ç¨³å®šçš„å½’ä¸€åŒ–æ–¹å¼
        theta_norm = theta_rad / (np.pi / 2)  # å½’ä¸€åŒ–åˆ°[-2, 2]èŒƒå›´ï¼Œç„¶åè£å‰ª
        phi_norm = phi_rad / np.pi  # å½’ä¸€åŒ–åˆ°[-1, 1]
        
        theta_norm = np.clip(theta_norm, -1, 1)
        phi_norm = np.clip(phi_norm, -1, 1)
        
        theta_label = torch.FloatTensor(theta_norm)  # [50]
        phi_label = torch.FloatTensor(phi_norm)      # [50]
        
        return H_input, theta_label, phi_label, H_complex

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class TransformerBeamformingNet(nn.Module):
    """åŸºäºTransformerçš„æ³¢æŸæˆå½¢ç½‘ç»œï¼Œæå‡è§’åº¦é¢„æµ‹ç²¾åº¦"""
    def __init__(self, input_dim=256*256*2, d_model=768, nhead=12, num_layers=8, dropout=0.15):
        super(TransformerBeamformingNet, self).__init__()
        
        self.d_model = d_model
        
        # é«˜æ•ˆçš„ç‰¹å¾æå–å™¨ï¼ˆä½¿ç”¨å·ç§¯é™ç»´ï¼‰
        self.feature_extractor = nn.Sequential(
            # å…ˆç”¨å·ç§¯é™ç»´å‡å°‘è®¡ç®—é‡
            nn.Conv1d(input_dim, d_model * 2, kernel_size=1),
            nn.BatchNorm1d(d_model * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Conv1d(d_model * 2, d_model, kernel_size=1),
            nn.BatchNorm1d(d_model),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_encoder = PositionalEncoding(d_model, max_len=100)
        
        # æ›´å¼ºçš„Transformerç¼–ç å™¨
        encoder_layers = TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,  # æ›´å¤§çš„FFN
            dropout=dropout,
            activation='gelu',  # ä½¿ç”¨GELUæ¿€æ´»
            batch_first=True,
            norm_first=True  # Pre-LNæ¶æ„ï¼Œæ›´ç¨³å®š
        )
        self.transformer_encoder = TransformerEncoder(
            encoder_layers, 
            num_layers=num_layers,
            norm=nn.LayerNorm(d_model)
        )
        
        # å¤šå¤´æ³¨æ„åŠ›èšåˆï¼ˆæ›¿ä»£ç®€å•çš„æœ€åæ—¶é—´æ­¥ï¼‰
        self.attention_pooling = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            batch_first=True
        )
        
        # æ”¹è¿›çš„è§’åº¦é¢„æµ‹å¤´ï¼ˆå¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.angle_predictor = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout * 1.5),  # æ›´é«˜çš„dropout
            nn.Linear(d_model, d_model // 2),
            nn.LayerNorm(d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout * 1.2),
            nn.Linear(d_model // 2, d_model // 4),
            nn.LayerNorm(d_model // 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 4, 64),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),  # è¾“å‡ºå±‚å‰è½»å¾®dropout
            nn.Linear(64, 2)  # theta, phi
        )
        
        # å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ç”¨äºæ³¨æ„åŠ›èšåˆ
        self.query_vector = nn.Parameter(torch.randn(1, 1, d_model))
        
        # æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨æ›´å¥½çš„åˆå§‹åŒ–
            nn.init.xavier_normal_(module.weight, gain=1.0)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
        elif isinstance(module, nn.Conv1d):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                    
    def forward(self, x):
        batch_size, channels, height, width, time_steps = x.shape
        
        # é‡å¡‘è¾“å…¥ [batch, time_steps, features]
        x = x.permute(0, 4, 1, 2, 3)  # [batch, time_steps, channels, height, width]
        x = x.reshape(batch_size, time_steps, -1)  # [batch, time_steps, features]
        
        # ç‰¹å¾æå–ï¼ˆä½¿ç”¨1Då·ç§¯ï¼‰
        x = x.permute(0, 2, 1)  # [batch, features, time_steps]
        x = self.feature_extractor(x)  # [batch, d_model, time_steps]
        x = x.permute(0, 2, 1)  # [batch, time_steps, d_model]
        
        # è¾“å…¥ç¼©æ”¾å’Œä½ç½®ç¼–ç 
        x = x * math.sqrt(self.d_model)
        x = x.transpose(0, 1)  # [time_steps, batch, d_model]
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)  # [batch, time_steps, d_model]
        
        # Transformerç¼–ç 
        encoded = self.transformer_encoder(x)  # [batch, time_steps, d_model]
        
        # æ³¨æ„åŠ›èšåˆï¼ˆæ›´å¥½çš„æ—¶åºä¿¡æ¯æ•´åˆï¼‰
        query = self.query_vector.expand(batch_size, -1, -1)  # [batch, 1, d_model]
        aggregated, attention_weights = self.attention_pooling(
            query, encoded, encoded
        )  # [batch, 1, d_model]
        aggregated = aggregated.squeeze(1)  # [batch, d_model]
        
        # è§’åº¦é¢„æµ‹
        angles = self.angle_predictor(aggregated)  # [batch, 2]
        
        return angles

def improved_loss_function(predictions, targets, loss_type='focal_huber'):
    """æ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œé™ä½è§’åº¦é¢„æµ‹è¯¯å·®"""
    pred_angles = predictions  # [batch, 2] (theta, phi)
    target_theta, target_phi = targets
    
    # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡
    target_theta_last = target_theta[:, -1]  # [batch]
    target_phi_last = target_phi[:, -1]      # [batch]
    
    if loss_type == 'focal_huber':
        # ç»„åˆFocal Losså’ŒHuber Lossçš„æ€æƒ³
        # è®¡ç®—åŸºç¡€è¯¯å·®
        theta_error = torch.abs(pred_angles[:, 0] - target_theta_last)
        phi_error = torch.abs(pred_angles[:, 1] - target_phi_last)
        
        # Focalæƒé‡ï¼šå¯¹éš¾æ ·æœ¬åŠ æƒ
        theta_focal_weight = torch.pow(theta_error + 1e-8, 0.5)  # å¹³æ–¹æ ¹æƒé‡
        phi_focal_weight = torch.pow(phi_error + 1e-8, 0.5)
        
        # HuberæŸå¤±
        theta_loss = F.huber_loss(pred_angles[:, 0], target_theta_last, delta=0.5, reduction='none')
        phi_loss = F.huber_loss(pred_angles[:, 1], target_phi_last, delta=0.5, reduction='none')
        
        # åº”ç”¨focalæƒé‡
        theta_loss = (theta_focal_weight * theta_loss).mean()
        phi_loss = (phi_focal_weight * phi_loss).mean()
        
    elif loss_type == 'adaptive_mse':
        # è‡ªé€‚åº”MSEæŸå¤±
        theta_loss = F.mse_loss(pred_angles[:, 0], target_theta_last)
        phi_loss = F.mse_loss(pred_angles[:, 1], target_phi_last)
        
        # åŠ¨æ€æƒé‡å¹³è¡¡
        theta_weight = 1.0 / (theta_loss.detach() + 1e-8)
        phi_weight = 1.0 / (phi_loss.detach() + 1e-8)
        
        total_weight = theta_weight + phi_weight
        theta_weight = theta_weight / total_weight
        phi_weight = phi_weight / total_weight
        
        theta_loss = theta_weight * theta_loss
        phi_loss = phi_weight * phi_loss
        
    else:
        # æ ‡å‡†HuberæŸå¤±
        theta_loss = F.huber_loss(pred_angles[:, 0], target_theta_last, delta=1.0)
        phi_loss = F.huber_loss(pred_angles[:, 1], target_phi_last, delta=1.0)
    
    total_loss = theta_loss + phi_loss
    
    # æ·»åŠ æ­£åˆ™åŒ–é¡¹
    l2_reg = 0.0
    for param in pred_angles.requires_grad_(True):
        if param.requires_grad:
            l2_reg += torch.norm(param)
    total_loss += 1e-6 * l2_reg
    
    # ç¡®ä¿æŸå¤±å€¼åœ¨åˆç†èŒƒå›´å†…
    total_loss = torch.clamp(total_loss, 0, 50)
    
    return total_loss, theta_loss, phi_loss

def train_model(model, train_loader, val_loader, num_epochs=150, lr=3e-4, device='cuda'):
    """æ”¹è¿›çš„è®­ç»ƒå‡½æ•°ï¼Œä½¿ç”¨æ›´é«˜å­¦ä¹ ç‡å’Œä¼˜åŒ–ç­–ç•¥"""
    model.to(device)
    
    # ä½¿ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=lr, 
        weight_decay=2e-4,  # å¢åŠ æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        eps=1e-8,
        betas=(0.9, 0.999)
    )
    
    # æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=lr,
        epochs=num_epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.1,  # 10%æ—¶é—´warm-up
        div_factor=10,  # åˆå§‹lr = max_lr/10
        final_div_factor=100,  # æœ€ç»ˆlr = max_lr/100
        anneal_strategy='cos'
    )
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 40  # å¢åŠ patienceï¼Œåœ¨ä½æŸå¤±æ—¶æ›´å®½å®¹
    min_improvement = 1e-5  # æœ€å°æ”¹è¿›é˜ˆå€¼ï¼Œé¿å…å¾®å°æ³¢åŠ¨è§¦å‘æ—©åœ
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§å­¦ä¹ ç‡: {lr}")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss_epoch = 0
        train_theta_loss = 0
        train_phi_loss = 0
        num_batches = 0
        
        train_progress = tqdm(train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}/{num_epochs}", leave=False)
        
        for batch_idx, (H_input, theta_label, phi_label, H_complex) in enumerate(train_progress):
            try:
                H_input = H_input.to(device)
                theta_label = theta_label.to(device)
                phi_label = phi_label.to(device)
                
                # æ£€æŸ¥è¾“å…¥æ•°æ®çš„æœ‰æ•ˆæ€§
                if torch.isnan(H_input).any() or torch.isinf(H_input).any():
                    print(f"è­¦å‘Š: è¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Infï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                predictions = model(H_input)
                
                # è®¡ç®—æ”¹è¿›çš„æŸå¤±
                loss, theta_loss, phi_loss = improved_loss_function(
                    predictions, (theta_label, phi_label), loss_type='focal_huber'
                )
                
                # æ£€æŸ¥æŸå¤±çš„æœ‰æ•ˆæ€§
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"è­¦å‘Š: æŸå¤±ä¸ºNaNæˆ–Infï¼Œè·³è¿‡æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ¢¯åº¦è£å‰ªï¼ˆé€‚åº”æ›´é«˜å­¦ä¹ ç‡ï¼‰
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
                
                optimizer.step()
                scheduler.step()  # OneCycleLRéœ€è¦æ¯ä¸ªstepè°ƒç”¨
                
                train_loss_epoch += loss.item()
                train_theta_loss += theta_loss.item()
                train_phi_loss += phi_loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                current_lr = optimizer.param_groups[0]['lr']
                train_progress.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Î¸': f'{theta_loss.item():.4f}',
                    'Ï†': f'{phi_loss.item():.4f}',
                    'LR': f'{current_lr:.2e}'
                })
                
            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss_epoch = 0
        val_theta_loss = 0
        val_phi_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            val_progress = tqdm(val_loader, desc="éªŒè¯", leave=False)
            for H_input, theta_label, phi_label, H_complex in val_progress:
                try:
                    H_input = H_input.to(device)
                    theta_label = theta_label.to(device)
                    phi_label = phi_label.to(device)
                    
                    predictions = model(H_input)
                    loss, theta_loss, phi_loss = improved_loss_function(
                        predictions, (theta_label, phi_label), loss_type='focal_huber'
                    )
                    
                    if not (torch.isnan(loss) or torch.isinf(loss)):
                        val_loss_epoch += loss.item()
                        val_theta_loss += theta_loss.item()
                        val_phi_loss += phi_loss.item()
                        val_batches += 1
                        
                except Exception as e:
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        if num_batches > 0:
            train_loss_avg = train_loss_epoch / num_batches
            train_theta_avg = train_theta_loss / num_batches
            train_phi_avg = train_phi_loss / num_batches
        else:
            train_loss_avg = float('inf')
            train_theta_avg = float('inf')
            train_phi_avg = float('inf')
            
        if val_batches > 0:
            val_loss_avg = val_loss_epoch / val_batches
            val_theta_avg = val_theta_loss / val_batches
            val_phi_avg = val_phi_loss / val_batches
        else:
            val_loss_avg = float('inf')
            val_theta_avg = float('inf')
            val_phi_avg = float('inf')
        
        train_losses.append(train_loss_avg)
        val_losses.append(val_loss_avg)
        
        # åœ¨æä½æŸå¤±æ—¶ä½¿ç”¨æ›´å®½æ¾çš„æ—©åœç­–ç•¥
        current_early_stop_patience = early_stop_patience
        if val_loss_avg < 0.001:  # å½“éªŒè¯æŸå¤±å¾ˆä½æ—¶
            current_early_stop_patience = 60  # æ›´å¤§çš„patience
        
        # æ‰“å°epochç»“æœ - å¢åŠ æ›´å¤šç›‘æ§ä¿¡æ¯
        current_lr = optimizer.param_groups[0]['lr']
        train_val_gap = val_loss_avg - train_loss_avg
        print(f"Epoch {epoch+1:2d}: Train={train_loss_avg:.6f} (Î¸:{train_theta_avg:.6f}, Ï†:{train_phi_avg:.6f}), "
              f"Val={val_loss_avg:.6f} (Î¸:{val_theta_avg:.6f}, Ï†:{val_phi_avg:.6f}), "
              f"Gap={train_val_gap:.6f}, LR={current_lr:.2e}, Patience={patience_counter}/{current_early_stop_patience}")
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜ - æ”¹è¿›ç­–ç•¥
        improvement = best_val_loss - val_loss_avg
        if improvement > min_improvement:  # åªæœ‰æ˜æ˜¾æ”¹è¿›æ‰é‡ç½®è®¡æ•°å™¨
            best_val_loss = val_loss_avg
            patience_counter = 0
            torch.save(model.state_dict(), 'best_beamforming_model_fixed.pth')
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss_avg:.6f}, æ”¹è¿›: {improvement:.6f})")
        else:
            patience_counter += 1
            
        if patience_counter >= current_early_stop_patience:
            print(f"æ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ (patience: {patience_counter}/{current_early_stop_patience})")
            break
    
    return train_losses, val_losses

def test_model(model, test_loader, device='cuda'):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    model.to(device)
    model.eval()
    
    all_predictions = []
    angle_errors = []
    
    with torch.no_grad():
        test_progress = tqdm(test_loader, desc="æµ‹è¯•è¿›åº¦")
        
        for H_input, theta_label, phi_label, H_complex in test_progress:
            try:
                H_input = H_input.to(device)
                theta_label = theta_label.to(device)
                phi_label = phi_label.to(device)
                
                # é¢„æµ‹
                predictions = model(H_input)
                
                # è½¬æ¢ä¸ºåº¦æ•°ï¼ˆåå½’ä¸€åŒ–ï¼‰
                pred_angles = predictions.cpu().numpy()
                pred_theta_norm = pred_angles[:, 0]  # [-1, 1]
                pred_phi_norm = pred_angles[:, 1]    # [-1, 1]
                
                # åå½’ä¸€åŒ–åˆ°åº¦æ•°ï¼ˆä¿®æ­£èŒƒå›´ï¼‰
                pred_theta_deg = pred_theta_norm * 90  # [-90, 90] å¯¹åº” [-1, 1]
                pred_phi_deg = pred_phi_norm * 180     # [-180, 180] å¯¹åº” [-1, 1]
                
                # ç›®æ ‡è§’åº¦ï¼ˆä½¿ç”¨æœ€åæ—¶é—´æ­¥ï¼‰
                target_theta_norm = theta_label[:, -1].cpu().numpy()
                target_phi_norm = phi_label[:, -1].cpu().numpy()
                
                target_theta_deg = target_theta_norm * 90
                target_phi_deg = target_phi_norm * 180
                
                # è®¡ç®—è§’åº¦è¯¯å·®
                theta_error = np.abs(pred_theta_deg - target_theta_deg)
                phi_error = np.abs(pred_phi_deg - target_phi_deg)
                
                # å¤„ç†phiè§’åº¦çš„å‘¨æœŸæ€§
                phi_error = np.minimum(phi_error, 360 - phi_error)
                
                total_error = theta_error + phi_error
                angle_errors.extend(total_error)
                
                for i in range(len(pred_angles)):
                    all_predictions.append({
                        'predicted_theta': pred_theta_deg[i],
                        'predicted_phi': pred_phi_deg[i],
                        'target_theta': target_theta_deg[i],
                        'target_phi': target_phi_deg[i],
                        'theta_error': theta_error[i],
                        'phi_error': phi_error[i],
                        'total_error': total_error[i]
                    })
                    
            except Exception as e:
                print(f"æµ‹è¯•æ‰¹æ¬¡é”™è¯¯: {e}")
                continue
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if len(angle_errors) > 0:
        mean_error = np.mean(angle_errors)
        std_error = np.std(angle_errors)
        median_error = np.median(angle_errors)
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"å¹³å‡è§’åº¦è¯¯å·®: {mean_error:.2f}Â°")
        print(f"è§’åº¦è¯¯å·®æ ‡å‡†å·®: {std_error:.2f}Â°")
        print(f"è§’åº¦è¯¯å·®ä¸­ä½æ•°: {median_error:.2f}Â°")
        print(f"æœ‰æ•ˆé¢„æµ‹æ•°é‡: {len(all_predictions)}")
        
        if len(all_predictions) > 0:
            theta_errors = [p['theta_error'] for p in all_predictions]
            phi_errors = [p['phi_error'] for p in all_predictions]
            print(f"Î¸è¯¯å·®: {np.mean(theta_errors):.2f}Â° Â± {np.std(theta_errors):.2f}Â°")
            print(f"Ï†è¯¯å·®: {np.mean(phi_errors):.2f}Â° Â± {np.std(phi_errors):.2f}Â°")
    else:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
    
    return all_predictions

def main():
    """ä¸»å‡½æ•°"""
    warnings.filterwarnings('ignore', category=UserWarning)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®è·¯å¾„
    channel_folder = "samples_data/channel_data_opt_20250702_143327"
    angle_folder = "samples_data/angle_data_opt_20250702_143327"
    
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = BeamformingDataset(channel_folder, angle_folder)
    
    # æŒ‰è¦æ±‚åˆ’åˆ†æ•°æ®é›†ï¼š800è®­ç»ƒï¼Œ200æµ‹è¯•
    total_samples = len(dataset)
    train_size = 800
    test_size = total_samples - train_size
    
    indices = list(range(total_samples))
    np.random.seed(42)  # è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
    np.random.shuffle(indices)
    
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]
    
    # ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›† - å¢åŠ éªŒè¯é›†å¤§å°æé«˜ç¨³å®šæ€§
    val_size = int(train_size * 0.25)  # 25%ä½œä¸ºéªŒè¯é›†ï¼Œæé«˜ç¨³å®šæ€§
    train_final_indices = train_indices[val_size:]
    val_indices = train_indices[:val_size]
    
    print(f"æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_final_indices)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬") 
    print(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®å­é›†
    train_dataset = torch.utils.data.Subset(dataset, train_final_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæé«˜æ‰¹æ¬¡å¤§å°åˆ©ç”¨Transformerå¹¶è¡Œæ€§ï¼‰
    batch_size = 6  # é€‚åº¦å¢åŠ æ‰¹æ¬¡å¤§å°
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)
    
    # åˆ›å»ºæ”¹è¿›çš„Transformeræ¨¡å‹
    model = TransformerBeamformingNet(
        input_dim=256*256*2,
        d_model=512,  # é€‚åº¦å‡å°æ¨¡å‹å®¹é‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        nhead=8,      # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
        num_layers=6, # å‡å°‘å±‚æ•°
        dropout=0.15  # å¢åŠ dropout
    )
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_losses, val_losses = train_model(
        model, train_loader, val_loader, 
        num_epochs=150,  # å¢åŠ è®­ç»ƒè½®æ•°
        lr=3e-4,         # æé«˜å­¦ä¹ ç‡
        device=device
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists('best_beamforming_model_fixed.pth'):
        model.load_state_dict(torch.load('best_beamforming_model_fixed.pth'))
        print("âœ“ åŠ è½½æœ€ä½³æ¨¡å‹å®Œæˆ")
    
    # æµ‹è¯•æ¨¡å‹
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•...")
    predictions = test_model(model, test_loader, device=device)
    
    # ç»˜åˆ¶ç»“æœ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # è®­ç»ƒæŸå¤±æ›²çº¿
    axes[0, 0].plot(train_losses, label='Training Loss', alpha=0.8)
    axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Progress')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_yscale('log')
    
    # è§’åº¦é¢„æµ‹æ•£ç‚¹å›¾
    if len(predictions) > 0:
        pred_theta = [p['predicted_theta'] for p in predictions]
        target_theta = [p['target_theta'] for p in predictions]
        pred_phi = [p['predicted_phi'] for p in predictions]
        target_phi = [p['target_phi'] for p in predictions]
        
        # Î¸è§’åº¦é¢„æµ‹
        axes[0, 1].scatter(target_theta, pred_theta, alpha=0.6, s=20)
        min_theta = min(min(target_theta), min(pred_theta))
        max_theta = max(max(target_theta), max(pred_theta))
        axes[0, 1].plot([min_theta, max_theta], [min_theta, max_theta], 'r--', label='Perfect')
        axes[0, 1].set_xlabel('True Î¸ (degrees)')
        axes[0, 1].set_ylabel('Predicted Î¸ (degrees)')
        axes[0, 1].set_title('Î¸ Angle Prediction')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Ï†è§’åº¦é¢„æµ‹
        axes[1, 0].scatter(target_phi, pred_phi, alpha=0.6, s=20)
        min_phi = min(min(target_phi), min(pred_phi))
        max_phi = max(max(target_phi), max(pred_phi))
        axes[1, 0].plot([min_phi, max_phi], [min_phi, max_phi], 'r--', label='Perfect')
        axes[1, 0].set_xlabel('True Ï† (degrees)')
        axes[1, 0].set_ylabel('Predicted Ï† (degrees)')
        axes[1, 0].set_title('Ï† Angle Prediction')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # è¯¯å·®åˆ†å¸ƒ
        errors = [p['total_error'] for p in predictions]
        axes[1, 1].hist(errors, bins=30, alpha=0.7, edgecolor='black')
        axes[1, 1].axvline(np.mean(errors), color='red', linestyle='--', label=f'Mean: {np.mean(errors):.1f}Â°')
        axes[1, 1].set_xlabel('Total Angle Error (degrees)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Error Distribution')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_results_fixed.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ è®­ç»ƒç»“æœå›¾è¡¨å·²ä¿å­˜ä¸º 'training_results_fixed.png'")
    
    # æ‰“å°è®­ç»ƒæ€»ç»“
    print(f"\nğŸ“‹ è®­ç»ƒæ€»ç»“:")
    if train_losses:
        print(f"â€¢ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
        print(f"â€¢ æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
        print(f"â€¢ æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}")
        print(f"â€¢ æŸå¤±æ”¹å–„: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%")
    
    return model, predictions

if __name__ == "__main__":
    main() 