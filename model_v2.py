import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import scipy.io as sio
import h5py
import os
import glob
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import math
from tqdm import tqdm
import warnings
from torch.cuda.amp import GradScaler
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import subprocess
import os
from torch.utils.checkpoint import checkpoint
import time

class BeamformingDataset(Dataset):
    """ä¼˜åŒ–ç‰ˆå¤©çº¿é˜µåˆ—ä¿¡é“æ•°æ®é›†"""
    def __init__(self, channel_folder, angle_folder, transform=None, use_augmentation=True):
        self.channel_folder = channel_folder
        self.angle_folder = angle_folder
        self.transform = transform
        self.use_augmentation = use_augmentation
        
        # è·å–ä¿¡é“æ•°æ®æ–‡ä»¶åˆ—è¡¨å¹¶æ’åº
        self.channel_files = sorted(glob.glob(os.path.join(channel_folder, '*.mat')))
        print(f"æ‰¾åˆ°ä¿¡é“æ•°æ®æ–‡ä»¶: {len(self.channel_files)} ä¸ª")
        
        # åŠ è½½è§’åº¦æ•°æ®ï¼ˆæ‰€æœ‰æ ·æœ¬çš„è§’åº¦ä¿¡æ¯åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼‰
        angle_files = glob.glob(os.path.join(angle_folder, '*.mat'))
        if len(angle_files) > 0:
            try:
                # å°è¯•ä½¿ç”¨ h5py è¯»å–
                with h5py.File(angle_files[0], 'r') as f:
                    all_theta = np.array(f['all_theta'])  # (50, 1000) or (1000, 50)
                    all_phi = np.array(f['all_phi'])      # (50, 1000) or (1000, 50)
                    
                    # æ£€æŸ¥ç»´åº¦å¹¶è°ƒæ•´
                    if all_theta.shape[0] == 50 and all_theta.shape[1] == 1000:
                        self.theta_array = all_theta.T  # (1000, 50)
                        self.phi_array = all_phi.T      # (1000, 50)
                    else:
                        self.theta_array = all_theta    # å‡è®¾å·²ç»æ˜¯(1000, 50)
                        self.phi_array = all_phi
                    
                    self.num_samples = self.theta_array.shape[0]
                    self.num_time_samples = self.theta_array.shape[1]
                    
                    print(f"è§’åº¦æ•°æ®å½¢çŠ¶: theta={self.theta_array.shape}, phi={self.phi_array.shape}")
                    
            except Exception as e:
                print(f"h5py è¯»å–å¤±è´¥: {e}")
                # ä½¿ç”¨ scipy.io è¯»å–
                try:
                    angle_data = sio.loadmat(angle_files[0])
                    # æ£€æŸ¥å¯èƒ½çš„é”®å
                    possible_keys = ['all_theta', 'theta_array', 'theta']
                    theta_key = None
                    phi_key = None
                    
                    for key in angle_data.keys():
                        if 'theta' in key.lower():
                            theta_key = key
                        if 'phi' in key.lower():
                            phi_key = key
                    
                    if theta_key and phi_key:
                        all_theta = angle_data[theta_key]
                        all_phi = angle_data[phi_key]
                        
                        if all_theta.shape[0] == 50 and all_theta.shape[1] == 1000:
                            self.theta_array = all_theta.T
                            self.phi_array = all_phi.T
                        else:
                            self.theta_array = all_theta
                            self.phi_array = all_phi
                            
                        self.num_samples = self.theta_array.shape[0]
                        self.num_time_samples = self.theta_array.shape[1]
                    else:
                        raise ValueError("æœªæ‰¾åˆ°thetaå’Œphiæ•°æ®")
                        
                except Exception as e2:
                    print(f"scipy.io è¯»å–ä¹Ÿå¤±è´¥: {e2}")
                    print(f"è§’åº¦æ–‡ä»¶å†…å®¹: {list(angle_data.keys()) if 'angle_data' in locals() else 'Unknown'}")
                    raise e2
        
        # ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œä½¿ç”¨è¾ƒå°çš„æ•°é‡
        self.actual_num_samples = min(len(self.channel_files), self.num_samples)
        print(f"å®é™…ä½¿ç”¨çš„æ ·æœ¬æ•°é‡: {self.actual_num_samples}")
        
    def __len__(self):
        return self.actual_num_samples
    
    def _add_data_augmentation(self, H_real, H_imag, theta_rad, phi_rad):
        """è½»é‡çº§æ•°æ®å¢å¼ºç­–ç•¥"""
        if not self.use_augmentation:
            return H_real, H_imag, theta_rad, phi_rad
            
        # 1. è½»å¾®å™ªå£°å¢å¼º
        noise_std = 0.005  # å‡å°å™ªå£°å¼ºåº¦
        H_real = H_real + np.random.normal(0, noise_std, H_real.shape)
        H_imag = H_imag + np.random.normal(0, noise_std, H_imag.shape)
        
        # 2. è§’åº¦å°å¹…æŠ–åŠ¨
        angle_noise_std = 0.01  # çº¦0.6åº¦çš„æ ‡å‡†å·®
        theta_rad = theta_rad + np.random.normal(0, angle_noise_std, theta_rad.shape)
        phi_rad = phi_rad + np.random.normal(0, angle_noise_std, phi_rad.shape)
        
        return H_real, H_imag, theta_rad, phi_rad
    
    def __getitem__(self, idx):
        if idx >= self.actual_num_samples:
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ {self.actual_num_samples}")
            
        # åŠ è½½ä¿¡é“æ•°æ®
        try:
            # å°è¯•ä½¿ç”¨ h5py è¯»å–
            with h5py.File(self.channel_files[idx], 'r') as f:
                H_real = np.array(f['H_real'])  # (50, 256, 256)
                H_imag = np.array(f['H_imag'])  # (50, 256, 256)
                
                # è½¬ç½®ä»¥åŒ¹é…é¢„æœŸæ ¼å¼ [256, 256, 50]
                H_real = H_real.transpose(1, 2, 0)  # (256, 256, 50)
                H_imag = H_imag.transpose(1, 2, 0)  # (256, 256, 50)
                
        except Exception as e:
            # ä½¿ç”¨ scipy.io è¯»å–
            try:
                channel_data = sio.loadmat(self.channel_files[idx])
                H_real = channel_data['H_real']  # [256, 256, 50]
                H_imag = channel_data['H_imag']
            except Exception as e2:
                print(f"æ— æ³•è¯»å–ä¿¡é“æ•°æ®æ–‡ä»¶ {self.channel_files[idx]}: {e2}")
                raise e2
        
        # è·å–è§’åº¦æ•°æ®
        theta_rad = self.theta_array[idx] * np.pi / 180  # è½¬æ¢ä¸ºå¼§åº¦
        phi_rad = self.phi_array[idx] * np.pi / 180
        
        # è½»é‡çº§æ•°æ®å¢å¼º
        H_real, H_imag, theta_rad, phi_rad = self._add_data_augmentation(
            H_real, H_imag, theta_rad, phi_rad
        )
        
        # ç»„åˆå®éƒ¨å’Œè™šéƒ¨
        H_complex = H_real + 1j * H_imag
        
        # æ›´é«˜æ•ˆçš„æ•°æ®å½’ä¸€åŒ–
        H_real_norm = H_real / (np.std(H_real) + 1e-8)
        H_imag_norm = H_imag / (np.std(H_imag) + 1e-8)
        
        # è£å‰ªæå€¼
        H_real_norm = np.clip(H_real_norm, -6, 6)
        H_imag_norm = np.clip(H_imag_norm, -6, 6)
        
        # è½¬æ¢ä¸ºPyTorch tensorï¼ˆä½¿ç”¨float16å‡å°‘å†…å­˜ï¼‰
        H_input = np.stack([H_real_norm, H_imag_norm], axis=0)  # [2, 256, 256, 50]
        H_input = torch.FloatTensor(H_input)
        
        # è§’åº¦æ ‡ç­¾å½’ä¸€åŒ–
        theta_norm = np.clip(theta_rad / (np.pi / 2), -1, 1)
        phi_norm = np.clip(phi_rad / np.pi, -1, 1)
        
        theta_label = torch.FloatTensor(theta_norm)  # [50]
        phi_label = torch.FloatTensor(phi_norm)      # [50]
        
        return H_input, theta_label, phi_label, H_complex

class MemoryEfficientPositionalEncoding(nn.Module):
    """å†…å­˜é«˜æ•ˆçš„ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=100):
        super(MemoryEfficientPositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        return x + self.pe[:x.size(1), :].unsqueeze(0)

class EfficientFeatureExtractor(nn.Module):
    """å†…å­˜é«˜æ•ˆçš„ç‰¹å¾æå–å™¨"""
    def __init__(self, input_dim, d_model, dropout=0.1):
        super(EfficientFeatureExtractor, self).__init__()
        
        # åˆ†é˜¶æ®µé™ç»´ï¼Œå¤§å¹…å‡å°‘ä¸­é—´ç»´åº¦
        intermediate_dims = [input_dim, input_dim//8, input_dim//32, d_model*2, d_model]
        
        self.feature_layers = nn.ModuleList()
        for i in range(len(intermediate_dims)-1):
            self.feature_layers.append(
                nn.Sequential(
                    nn.Conv1d(intermediate_dims[i], intermediate_dims[i+1], kernel_size=1),
                    nn.BatchNorm1d(intermediate_dims[i+1]),
                    nn.GELU(),
                    nn.Dropout(dropout)
                )
            )
        
    def forward(self, x):
        for layer in self.feature_layers:
            x = layer(x)
        return x

class OptimizedTransformerBeamformingNet(nn.Module):
    """å†…å­˜ä¼˜åŒ–çš„Transformeræ³¢æŸæˆå½¢ç½‘ç»œ"""
    def __init__(self, input_dim=256*256*2, d_model=512, nhead=8, num_layers=6, dropout=0.1):
        super(OptimizedTransformerBeamformingNet, self).__init__()
        
        self.d_model = d_model
        
        # å¤§å¹…å‡å°çš„ç‰¹å¾æå–å™¨
        self.feature_extractor = EfficientFeatureExtractor(
            input_dim=input_dim,
            d_model=d_model,
            dropout=dropout
        )
        
        # ä½ç½®ç¼–ç 
        self.pos_encoder = MemoryEfficientPositionalEncoding(d_model, max_len=100)
        
        # è½»é‡çº§Transformerç¼–ç å™¨
        encoder_layer = TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 2,  # å‡å°FFNå¤§å°
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers,
            norm=nn.LayerNorm(d_model)
        )
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›èšåˆ
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # è½»é‡çº§é¢„æµ‹å¤´
        self.predictor = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.LayerNorm(d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(d_model // 4, 2)  # ç›´æ¥è¾“å‡ºthetaå’Œphi
        )
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight, gain=0.5)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
                    
    def forward(self, x):
        batch_size, channels, height, width, time_steps = x.shape
        
        # é‡å¡‘è¾“å…¥ [batch, time_steps, features]
        x = x.permute(0, 4, 1, 2, 3)  # [batch, time_steps, channels, height, width]
        x = x.reshape(batch_size, time_steps, -1)  # [batch, time_steps, features]
        
        # ç‰¹å¾æå–
        x = x.permute(0, 2, 1)  # [batch, features, time_steps]
        x = self.feature_extractor(x)  # [batch, d_model, time_steps]
        x = x.permute(0, 2, 1)  # [batch, time_steps, d_model]
        
        # ä½ç½®ç¼–ç 
        x = x * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        
        # Transformerç¼–ç ï¼ˆç§»é™¤æ¢¯åº¦æ£€æŸ¥ç‚¹é¿å…DDPé—®é¢˜ï¼‰
        encoded = self.transformer_encoder(x)
        
        # å…¨å±€æ± åŒ–
        encoded = encoded.permute(0, 2, 1)  # [batch, d_model, time_steps]
        pooled = self.global_pool(encoded).squeeze(-1)  # [batch, d_model]
        
        # é¢„æµ‹
        angles = self.predictor(pooled)  # [batch, 2]
        
        return angles

class FocalHuberLoss(nn.Module):
    """ç®€åŒ–çš„æŸå¤±å‡½æ•°"""
    def __init__(self, alpha=2.0, delta=0.5):
        super(FocalHuberLoss, self).__init__()
        self.alpha = alpha
        self.delta = delta
        
    def forward(self, predictions, targets):
        target_theta, target_phi = targets
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡
        target_theta_last = target_theta[:, -1]  # [batch]
        target_phi_last = target_phi[:, -1]      # [batch]
        
        # HuberæŸå¤±
        theta_loss = F.huber_loss(predictions[:, 0], target_theta_last, delta=self.delta)
        phi_loss = F.huber_loss(predictions[:, 1], target_phi_last, delta=self.delta)
        
        total_loss = self.alpha * (theta_loss + phi_loss)
        
        return total_loss, theta_loss, phi_loss

def setup_distributed_training():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨multiprocessing.spawnæ–¹å¼
    if os.environ.get('USE_MP_SPAWN') == '1' and torch.cuda.device_count() > 1:
        print("ä½¿ç”¨ multiprocessing.spawn å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['WORLD_SIZE'] = str(torch.cuda.device_count())
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
        
        # å¯åŠ¨å¤šè¿›ç¨‹
        if not os.environ.get('SPAWN_STARTED'):
            os.environ['SPAWN_STARTED'] = '1'
            mp.spawn(
                distributed_main_spawn,
                args=(),
                nprocs=torch.cuda.device_count(),
                join=True
            )
            return None, None, None, True
    
    # å¦‚æœæ˜¯å•æœºå¤šå¡ï¼Œè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
    if 'WORLD_SIZE' not in os.environ and torch.cuda.device_count() > 1:
        print(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œè‡ªåŠ¨å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        
        # è‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['WORLD_SIZE'] = str(torch.cuda.device_count())
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
        
        # å¯åŠ¨å…¶ä»–è¿›ç¨‹
        if os.environ.get('AUTO_DISTRIBUTED') != 'True':
            os.environ['AUTO_DISTRIBUTED'] = 'True'
            
            # ä½¿ç”¨torch.multiprocessingå¯åŠ¨å¤šè¿›ç¨‹
            mp.spawn(
                distributed_main,
                args=(),
                nprocs=torch.cuda.device_count(),
                join=True
            )
            return None, None, None, True  # æ ‡è®°ä¸ºå·²å¤„ç†
    
    if 'WORLD_SIZE' in os.environ:
        world_size = int(os.environ['WORLD_SIZE'])
        rank = int(os.environ.get('RANK', 0))
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        print(f"åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ: world_size={world_size}, rank={rank}, local_rank={local_rank}")
        
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(local_rank)
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        if not dist.is_initialized():
            try:
                dist.init_process_group(
                    backend='nccl',
                    init_method='env://',
                    world_size=world_size,
                    rank=rank,
                    timeout=torch.distributed.constants.default_pg_timeout
                )
                print(f"Rank {rank}: åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"åˆ†å¸ƒå¼åˆå§‹åŒ–å¤±è´¥: {e}")
                return 1, 0, 0, False
        
        return world_size, rank, local_rank, False
    else:
        return 1, 0, 0, False

def distributed_main_spawn(rank):
    """ä½¿ç”¨spawnæ–¹å¼çš„åˆ†å¸ƒå¼è®­ç»ƒä¸»å‡½æ•°"""
    os.environ['RANK'] = str(rank)
    os.environ['LOCAL_RANK'] = str(rank)
    
    # é˜²æ­¢é€’å½’è°ƒç”¨
    if 'SPAWN_STARTED' in os.environ:
        del os.environ['USE_MP_SPAWN']
    
    main()

def distributed_main(rank):
    """åˆ†å¸ƒå¼è®­ç»ƒä¸»å‡½æ•°"""
    os.environ['RANK'] = str(rank)
    os.environ['LOCAL_RANK'] = str(rank)
    
    main()

def train_optimized_model(model, train_loader, val_loader, num_epochs=150, lr=1e-4, 
                         device='cuda', use_distributed=False, world_size=1, rank=0, local_rank=0):
    """ä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒå‡½æ•°"""
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        
    model.to(device)
    
    # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
    if use_distributed and dist.is_initialized() and world_size > 1:
        model = DDP(
            model, 
            device_ids=[local_rank], 
            output_device=local_rank,
            find_unused_parameters=False,  # å…ˆè®¾ä¸ºFalseï¼Œå¼ºåˆ¶æ‰€æœ‰å‚æ•°å‚ä¸è®¡ç®—
            broadcast_buffers=False,      # å‡å°‘é€šä¿¡å¼€é”€
            gradient_as_bucket_view=True, # å†…å­˜ä¼˜åŒ–
            static_graph=True             # ä½¿ç”¨é™æ€å›¾ä¼˜åŒ–
        )
        print(f"Rank {rank}: æ¨¡å‹å·²åŒ…è£…ä¸ºDDPï¼ˆé™æ€å›¾æ¨¡å¼ï¼‰")
    
    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler()
    
    # è°ƒæ•´å­¦ä¹ ç‡
    effective_lr = lr * world_size if use_distributed and world_size > 1 else lr
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=effective_lr, 
        weight_decay=5e-5,  # å‡å°æƒé‡è¡°å‡
        eps=1e-8
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=20,
        T_mult=2,
        eta_min=effective_lr * 0.01
    )
    
    # ç®€åŒ–çš„æŸå¤±å‡½æ•°
    criterion = FocalHuberLoss(alpha=2.0, delta=0.5)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    early_stop_patience = 30
    min_improvement = 1e-6  # æœ€å°æ”¹è¿›é˜ˆå€¼
    
    # åˆ†å¸ƒå¼è®­ç»ƒçš„æ¢¯åº¦ç´¯ç§¯è®¾ç½®
    accumulation_steps = 4 if not use_distributed else 2  # åˆ†å¸ƒå¼æ—¶å‡å°‘ç´¯ç§¯æ­¥æ•°
    
    is_main_process = rank == 0
    if is_main_process:
        print(f"å¼€å§‹ä¼˜åŒ–æ¨¡å‹è®­ç»ƒï¼Œæœ‰æ•ˆå­¦ä¹ ç‡: {effective_lr}")
        if use_distributed and world_size > 1:
            print(f"ä½¿ç”¨ {world_size} ä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
            print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {accumulation_steps}")
    
    for epoch in range(num_epochs):
        # è®¾ç½®åˆ†å¸ƒå¼samplerçš„epoch
        if hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(epoch)
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss_epoch = 0
        num_batches = 0
        
        train_progress = tqdm(train_loader, desc=f"è®­ç»ƒ Epoch {epoch+1}/{num_epochs}", 
                             leave=False, disable=not is_main_process)
        
        # åˆå§‹åŒ–æ¢¯åº¦
        optimizer.zero_grad()
        
        for batch_idx, (H_input, theta_label, phi_label, H_complex) in enumerate(train_progress):
            try:
                H_input = H_input.to(device, non_blocking=True)
                theta_label = theta_label.to(device, non_blocking=True)
                phi_label = phi_label.to(device, non_blocking=True)
                
                # æ£€æŸ¥è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
                if torch.isnan(H_input).any() or torch.isinf(H_input).any():
                    if is_main_process:
                        print(f"è·³è¿‡æ— æ•ˆæ•°æ®æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                # è®¡ç®—æ˜¯å¦æ˜¯ç´¯ç§¯çš„æœ€åä¸€æ­¥
                is_accumulation_step = (batch_idx + 1) % accumulation_steps == 0
                is_last_batch = batch_idx == len(train_loader) - 1
                should_step = is_accumulation_step or is_last_batch
                
                # å‰å‘ä¼ æ’­
                with torch.amp.autocast('cuda'):
                    predictions = model(H_input)
                    loss, theta_loss, phi_loss = criterion(predictions, (theta_label, phi_label))
                    
                    # å¯¹åˆ†å¸ƒå¼è®­ç»ƒè¿›è¡Œlossç¼©æ”¾
                    if use_distributed and world_size > 1:
                        loss = loss / world_size
                    loss = loss / accumulation_steps
                
                # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
                if torch.isnan(loss) or torch.isinf(loss):
                    if is_main_process:
                        print(f"è·³è¿‡æ— æ•ˆæŸå¤±æ‰¹æ¬¡ {batch_idx}")
                    continue
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # åœ¨ç´¯ç§¯å®Œæˆæ—¶æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
                if should_step:
                    # æ¢¯åº¦è£å‰ª
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                    optimizer.zero_grad()
                
                train_loss_epoch += loss.item() * accumulation_steps
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                train_progress.set_postfix({
                    'Loss': f'{loss.item() * accumulation_steps:.5f}',
                    'LR': f'{optimizer.param_groups[0]["lr"]:.2e}',
                    'Step': f'{batch_idx % accumulation_steps + 1}/{accumulation_steps}'
                })
                
                # å®šæœŸæ¸…ç†ç¼“å­˜
                if batch_idx % 20 == 0:
                    torch.cuda.empty_cache()
                
            except Exception as e:
                if is_main_process:
                    print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                
                # å‘ç”Ÿé”™è¯¯æ—¶é‡ç½®æ¢¯åº¦
                optimizer.zero_grad()
                torch.cuda.empty_cache()
                continue
        
        # epochç»“æŸæ—¶ç¡®ä¿æ¢¯åº¦å·²æ¸…ç†
        optimizer.zero_grad()
        
        # åŒæ­¥åˆ†å¸ƒå¼è®­ç»ƒçš„æŸå¤±ç»Ÿè®¡
        if use_distributed and dist.is_initialized() and world_size > 1:
            # åˆ›å»ºtensorå¹¶è¿›è¡Œall_reduce
            train_loss_tensor = torch.tensor(train_loss_epoch, device=device, dtype=torch.float32)
            num_batches_tensor = torch.tensor(num_batches, device=device, dtype=torch.float32)
            
            dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(num_batches_tensor, op=dist.ReduceOp.SUM)
            
            train_loss_epoch = train_loss_tensor.item()
            num_batches = int(num_batches_tensor.item())
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss_epoch = 0
        val_batches = 0
        
        with torch.no_grad():
            val_progress = tqdm(val_loader, desc="éªŒè¯", leave=False, disable=not is_main_process)
            for H_input, theta_label, phi_label, H_complex in val_progress:
                try:
                    H_input = H_input.to(device, non_blocking=True)
                    theta_label = theta_label.to(device, non_blocking=True)
                    phi_label = phi_label.to(device, non_blocking=True)
                    
                    with torch.amp.autocast('cuda'):
                        predictions = model(H_input)
                        loss, _, _ = criterion(predictions, (theta_label, phi_label))
                        
                        # å¯¹åˆ†å¸ƒå¼è®­ç»ƒè¿›è¡Œlossç¼©æ”¾
                        if use_distributed and world_size > 1:
                            loss = loss / world_size
                    
                    if not (torch.isnan(loss) or torch.isinf(loss)):
                        val_loss_epoch += loss.item()
                        val_batches += 1
                        
                except Exception as e:
                    if is_main_process:
                        print(f"éªŒè¯æ‰¹æ¬¡é”™è¯¯: {e}")
                    continue
        
        # åŒæ­¥éªŒè¯æŸå¤±
        if use_distributed and dist.is_initialized() and world_size > 1:
            val_loss_tensor = torch.tensor(val_loss_epoch, device=device, dtype=torch.float32)
            val_batches_tensor = torch.tensor(val_batches, device=device, dtype=torch.float32)
            
            dist.all_reduce(val_loss_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(val_batches_tensor, op=dist.ReduceOp.SUM)
            
            val_loss_epoch = val_loss_tensor.item()
            val_batches = int(val_batches_tensor.item())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        train_loss_avg = train_loss_epoch / max(num_batches, 1)
        val_loss_avg = val_loss_epoch / max(val_batches, 1)
        
        train_losses.append(train_loss_avg)
        val_losses.append(val_loss_avg)
        
        # æ‰“å°ç»“æœ
        if is_main_process:
            print(f"Epoch {epoch+1:3d}: Train={train_loss_avg:.6f}, Val={val_loss_avg:.6f}, "
                  f"LR={optimizer.param_groups[0]['lr']:.2e}")
        
        # æ—©åœå’Œæ¨¡å‹ä¿å­˜ (åªåœ¨ä¸»è¿›ç¨‹)
        improvement = best_val_loss - val_loss_avg
        if improvement > min_improvement:
            best_val_loss = val_loss_avg
            patience_counter = 0
            if is_main_process:
                # ä¿å­˜æ¨¡å‹æ—¶éœ€è¦å¤„ç†DDPå’ŒDataParallelåŒ…è£…
                if hasattr(model, 'module'):
                    # DDPæˆ–DataParallelåŒ…è£…çš„æ¨¡å‹
                    model_to_save = model.module
                else:
                    # æ™®é€šæ¨¡å‹
                    model_to_save = model
                    
                torch.save(model_to_save.state_dict(), 'best_optimized_model.pth')
                print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss_avg:.6f}, æ”¹è¿›: {improvement:.6f})")
        else:
            patience_counter += 1
            
        if patience_counter >= early_stop_patience:
            if is_main_process:
                print(f"æ—©åœè§¦å‘ï¼åœæ­¢è®­ç»ƒ")
            break
    
    return train_losses, val_losses

def test_optimized_model(model, test_loader, device='cuda'):
    """æµ‹è¯•ä¼˜åŒ–æ¨¡å‹"""
    model.to(device)
    model.eval()
    
    all_predictions = []
    angle_errors = []
    
    with torch.no_grad():
        for H_input, theta_label, phi_label, H_complex in tqdm(test_loader, desc="æµ‹è¯•æ¨¡å‹"):
            try:
                H_input = H_input.to(device)
                theta_label = theta_label.to(device)
                phi_label = phi_label.to(device)
                
                with torch.amp.autocast('cuda'):
                    predictions = model(H_input)
                
                # è½¬æ¢ä¸ºåº¦æ•°
                pred_angles = predictions.cpu().numpy()
                pred_theta_deg = pred_angles[:, 0] * 90
                pred_phi_deg = pred_angles[:, 1] * 180
                
                # ç›®æ ‡è§’åº¦
                target_theta_deg = theta_label[:, -1].cpu().numpy() * 90
                target_phi_deg = phi_label[:, -1].cpu().numpy() * 180
                
                # è®¡ç®—è¯¯å·®
                theta_error = np.abs(pred_theta_deg - target_theta_deg)
                phi_error = np.abs(pred_phi_deg - target_phi_deg)
                phi_error = np.minimum(phi_error, 360 - phi_error)
                
                total_error = theta_error + phi_error
                angle_errors.extend(total_error)
                
                for i in range(len(pred_angles)):
                    all_predictions.append({
                        'predicted_theta': pred_theta_deg[i],
                        'predicted_phi': pred_phi_deg[i],
                        'target_theta': target_theta_deg[i],
                        'target_phi': target_phi_deg[i],
                        'theta_error': theta_error[i],
                        'phi_error': phi_error[i],
                        'total_error': total_error[i]
                    })
                    
            except Exception as e:
                print(f"æµ‹è¯•æ‰¹æ¬¡é”™è¯¯: {e}")
                continue
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if len(angle_errors) > 0:
        mean_error = np.mean(angle_errors)
        std_error = np.std(angle_errors)
        median_error = np.median(angle_errors)
        
        print(f"\nğŸ“Š ä¼˜åŒ–æ¨¡å‹æµ‹è¯•ç»“æœ:")
        print(f"å¹³å‡è§’åº¦è¯¯å·®: {mean_error:.3f}Â°")
        print(f"è§’åº¦è¯¯å·®æ ‡å‡†å·®: {std_error:.3f}Â°")
        print(f"è§’åº¦è¯¯å·®ä¸­ä½æ•°: {median_error:.3f}Â°")
        print(f"æœ‰æ•ˆé¢„æµ‹æ•°é‡: {len(all_predictions)}")
    
    return all_predictions

def main():
    """ä¸»å‡½æ•°"""
    warnings.filterwarnings('ignore', category=UserWarning)
    
    # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DataParallelæ¨¡å¼
    use_dataparallel = os.environ.get('USE_DATAPARALLEL', '0') == '1'
    
    if use_dataparallel:
        # DataParallelæ¨¡å¼ï¼šå•è¿›ç¨‹å¤šGPU
        print("ä½¿ç”¨ DataParallel æ¨¡å¼")
        world_size, rank, local_rank, already_handled = 1, 0, 0, False
        use_distributed = False
        
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            device = 'cuda:0'
            print(f"DataParallel å°†ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
        else:
            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    else:
        # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
        world_size, rank, local_rank, already_handled = setup_distributed_training()
        if already_handled:
            return  # å¦‚æœå·²ç»é€šè¿‡multiprocessingå¤„ç†äº†ï¼Œç›´æ¥è¿”å›
        
        use_distributed = world_size > 1 and dist.is_initialized()
        
        # è®¾å¤‡é…ç½®
        if torch.cuda.is_available():
            if local_rank is not None:
                device = f'cuda:{local_rank}'
                torch.cuda.set_device(local_rank)
            else:
                device = 'cuda:0'
        else:
            device = 'cpu'
    
    if rank == 0:
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        if torch.cuda.is_available():
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"GPU {i} å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
    
    # æ•°æ®è·¯å¾„
    channel_folder = "samples_data/channel_data_opt_20250702_143327"
    angle_folder = "samples_data/angle_data_opt_20250702_143327"
    
    if rank == 0:
        print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = BeamformingDataset(channel_folder, angle_folder, use_augmentation=True)
    
    # æ•°æ®é›†åˆ’åˆ†
    total_samples = len(dataset)
    train_size = 800
    test_size = total_samples - train_size
    
    indices = list(range(total_samples))
    np.random.seed(42)
    np.random.shuffle(indices)
    
    train_indices = indices[:train_size]
    test_indices = indices[train_size:]
    
    val_size = int(train_size * 0.20)
    train_final_indices = train_indices[val_size:]
    val_indices = train_indices[:val_size]
    
    if rank == 0:
        print(f"ä¼˜åŒ–æ¨¡å‹æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_final_indices)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬") 
        print(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®å­é›†
    train_dataset = torch.utils.data.Subset(dataset, train_final_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)
    test_dataset = torch.utils.data.Subset(dataset, test_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 2  # åŸºç¡€æ‰¹æ¬¡å¤§å°
    
    # DataParallelæ¨¡å¼ä¸‹å¯ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°
    if use_dataparallel and torch.cuda.device_count() > 1:
        batch_size = 2 * torch.cuda.device_count()
        print(f"DataParallelæ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º: {batch_size}")
    
    train_sampler = None
    val_sampler = None
    test_sampler = None
    
    # åªæœ‰çœŸæ­£çš„åˆ†å¸ƒå¼è®­ç»ƒæ‰ä½¿ç”¨DistributedSampler
    if use_distributed:
        train_sampler = DistributedSampler(train_dataset)
        val_sampler = DistributedSampler(val_dataset, shuffle=False)
        test_sampler = DistributedSampler(test_dataset, shuffle=False)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=(train_sampler is None), 
        sampler=train_sampler,
        num_workers=2,
        pin_memory=True, 
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        sampler=val_sampler,
        num_workers=2, 
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        sampler=test_sampler,
        num_workers=2, 
        pin_memory=True
    )
    
    # åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
    model = OptimizedTransformerBeamformingNet(
        input_dim=256*256*2,
        d_model=512,
        nhead=8,
        num_layers=6,
        dropout=0.1
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    if rank == 0:
        print(f"ä¼˜åŒ–æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    
    # å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡å¹¶è®¾ç½®å¹¶è¡Œæ¨¡å¼
    model.to(device)
    
    if use_dataparallel and torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ DataParallel åŒ…è£…æ¨¡å‹ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        model = nn.DataParallel(model)
        use_distributed_training_flag = False
    elif use_distributed:
        use_distributed_training_flag = True
    else:
        use_distributed_training_flag = False
    
    # è®­ç»ƒæ¨¡å‹
    if rank == 0:
        print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–æ¨¡å‹è®­ç»ƒ ({'DataParallel' if use_dataparallel else 'DDP' if use_distributed else 'Single GPU'})...")
    
    train_losses, val_losses = train_optimized_model(
        model, train_loader, val_loader, 
        num_epochs=150,
        lr=1e-4,
        device=device,
        use_distributed=use_distributed_training_flag,
        world_size=world_size,
        rank=rank,
        local_rank=local_rank
    )
    
    # åªåœ¨ä¸»è¿›ç¨‹è¿›è¡Œæµ‹è¯•å’Œå¯è§†åŒ–
    if rank == 0:
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if os.path.exists('best_optimized_model.pth'):
            # å¤„ç†DataParallelä¿å­˜çš„æ¨¡å‹
            checkpoint = torch.load('best_optimized_model.pth')
            if isinstance(model, nn.DataParallel):
                model.module.load_state_dict(checkpoint)
            else:
                model.load_state_dict(checkpoint)
            print("âœ“ åŠ è½½æœ€ä½³ä¼˜åŒ–æ¨¡å‹å®Œæˆ")
        
        # æµ‹è¯•æ¨¡å‹
        print("\nğŸ§ª å¼€å§‹ä¼˜åŒ–æ¨¡å‹æµ‹è¯•...")
        predictions = test_optimized_model(model, test_loader, device=device)
        
        # ç»˜åˆ¶ç»“æœ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # è®­ç»ƒæŸå¤±æ›²çº¿
        axes[0, 0].plot(train_losses, label='Training Loss', alpha=0.8)
        axes[0, 0].plot(val_losses, label='Validation Loss', alpha=0.8)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Optimized Model Training Progress')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_yscale('log')
        
        if len(predictions) > 0:
            pred_theta = [p['predicted_theta'] for p in predictions]
            target_theta = [p['target_theta'] for p in predictions]
            pred_phi = [p['predicted_phi'] for p in predictions]
            target_phi = [p['target_phi'] for p in predictions]
            
            # Î¸è§’åº¦é¢„æµ‹
            axes[0, 1].scatter(target_theta, pred_theta, alpha=0.6, s=25)
            min_theta = min(min(target_theta), min(pred_theta))
            max_theta = max(max(target_theta), max(pred_theta))
            axes[0, 1].plot([min_theta, max_theta], [min_theta, max_theta], 'r--', label='Perfect')
            axes[0, 1].set_xlabel('True Î¸ (degrees)')
            axes[0, 1].set_ylabel('Predicted Î¸ (degrees)')
            axes[0, 1].set_title('Î¸ Angle Prediction (Optimized)')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # Ï†è§’åº¦é¢„æµ‹
            axes[1, 0].scatter(target_phi, pred_phi, alpha=0.6, s=25)
            min_phi = min(min(target_phi), min(pred_phi))
            max_phi = max(max(target_phi), max(pred_phi))
            axes[1, 0].plot([min_phi, max_phi], [min_phi, max_phi], 'r--', label='Perfect')
            axes[1, 0].set_xlabel('True Ï† (degrees)')
            axes[1, 0].set_ylabel('Predicted Ï† (degrees)')
            axes[1, 0].set_title('Ï† Angle Prediction (Optimized)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            # è¯¯å·®åˆ†å¸ƒ
            errors = [p['total_error'] for p in predictions]
            axes[1, 1].hist(errors, bins=30, alpha=0.7, edgecolor='black')
            axes[1, 1].axvline(np.mean(errors), color='red', linestyle='--',
                              label=f'Mean: {np.mean(errors):.2f}Â°')
            axes[1, 1].set_xlabel('Total Angle Error (degrees)')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Error Distribution (Optimized)')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('optimized_training_results.png', dpi=300, bbox_inches='tight')
        print(f"\nğŸ“ˆ ä¼˜åŒ–æ¨¡å‹è®­ç»ƒç»“æœå›¾è¡¨å·²ä¿å­˜")
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ“‹ ä¼˜åŒ–æ¨¡å‹è®­ç»ƒæ€»ç»“:")
        if train_losses:
            print(f"â€¢ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
            print(f"â€¢ æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
            print(f"â€¢ æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}")
            print(f"â€¢ æ¨¡å‹è§„æ¨¡: {total_params/1e6:.1f}M å‚æ•°")
            
            training_mode = "DataParallel" if use_dataparallel else "DDP" if use_distributed else "Single GPU"
            print(f"â€¢ è®­ç»ƒæ¨¡å¼: {training_mode}")
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if use_distributed:
        dist.destroy_process_group()

if __name__ == "__main__":
    main()